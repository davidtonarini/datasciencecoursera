---
title: "Prediction assignment"
author: "David Tonarini"
date: "19/02/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modelling quality of human activity

Human activity recognition research has traditionally focused on a quantitative assesment of different activities, i.e. to predict "which" activity was performed, "how much", and so on. 
This analysis focuses on the creation of a model to make qualitative assesment of human activity, in the context of Weight Lifting. 

The used dataset collected data about six young health participants whoe performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: one correct, and four common mistakes. 
Data are assigned to a class based on the execution: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

This is the variable "classe" of the dataset. I am going to do some exploratory analysis and build a prediction model based on this dataset. 

These parameters are used to load the dataset, and the caret package is used to split the data into a training and validation section. And additional dataset of 20 test cases is also loaded and put aside. At the end of the analysis, the machine learning algorithm will be applied once to the test cases. 

```{r prepare }

#prepare libraries
library(caret)
library(ggplot2)
set.seed(666);

training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")


```


## Exploratory data analysis

```{r na}
full_rows <- training[ complete.cases(training), ]
```


I tried restricting the dataset to only complete cases for a better analysis, but doing so results in a dataset of only `r dim(full_rows)[1]` cases. 

Therefore, I decided to deal with the missing values column wise. Various columns contain almost all NAs. As such, I selected the columns where the NAs make 90% or more of the values, and excluded them from the analysis.

Additionally, with a first glance at the dataset we can notice that multiple columns hold informations about the user, the timestamp, or other details which are irrelavent to the prediction algorythm.
All these useless columns will be excluded first.


```{r cleaning_data}
#Irrelevant columns
useless_cols <- c("X", "user_name", "raw_timestamp_part_1", 
    "raw_timestamp_part_2","cvtd_timestamp","new_window", "num_window") 
#Missing values
row_count <- dim(training)[1]
for (i in 8:160) {
  #check amount of NAs in that columns
  na_sum <- sum(is.na(training[,i]))
  if (na_sum/row_count > 0.9 ) {
    useless_cols <- c(useless_cols, colnames(training)[i])
  } 
}


#remove them
training <- training[ , !(names(training) %in% useless_cols)]
testing <- testing[ , !(names(testing) %in% useless_cols)]

```

This resulted in removing `r length(useless_cols)` columns, leaving use with a dataset with `r dim(training)[2] -1 ` potential predictor aside from the "classe" variable we are interested into.

## Pre-processing

I decided to pre-process the data with Principal Component Analysis. By default, the PCA function in caret retains enough columns to explain 95% of the observed variance. However, I realised that on my old machine that resulted in an unmanageable computation load. As such, I decided to set the pre-processing to only pick the 10 most influential predictors. 
This will result in less accuracy, but will make the software computationally light enough to run on my machine.


```{r preprocessing}
#Save classe data for later
y <- training$classe 

#Prepare data without classe variable
classe_index <- which( colnames(training)=="classe" )
x_training <- data.frame( data.matrix( training[ , -classe_index] ) )
x_testing <- data.frame( data.matrix( testing ) )

# Scaling and centeing
scaled <- preProcess(x_training, method=c("center","scale"))
x_training <- predict(scaled, x_training)
x_testing <- predict(scaled, x_testing)

#Pre-processing with PCA
pp <- preProcess(x_training, method="pca", pcaComp = 10)
pca <- predict(pp, x_training)
pca_test <- predict(pp, x_testing)
```


After this, I used the caret package to divide the dataset in a training and a validation set. For the same reason above, a relatively small part of the dataset is used for training.

```{r modelling}
indexes = createDataPartition( training$classe, p = 0.1, list=FALSE)

sub_training <- pca[ indexes, ]
sub_validation <- pca[ -indexes, ]


fitControl <- trainControl(method = "cv",
                            number = 5,
                            allowParallel = TRUE)

model <- train(sub_training, y[indexes], method='rf', na.action = na.omit, 
                proxy=TRUE, trControl = fitControl)
  
```


The created model uses random forest algorithm to create a prediction. I applied the model to predict the data set apart for validation. Results from the confusion matrix is shown below

```{r confusionmatrix}
confusionMatrix( predict(model, pca[ -indexes, ] ), y [ -indexes ]  )
```


I am considering the accuracy of the model satisfactory, given the restrictions on the design. 
Different models could be reaching better predictive outcomes, but this model has the advantage of low computational demand. Further analysis on the data is however recommended to tune the model, and a higher number of component and larger portion of training data are recommended on suitable machine.


## Application to the test cases

The model is applied to the 20 test cases. The accuracy on the dataset will be analysed after the results are publishd.

```{r test_cases, echo = FALSE}

#test_cases <- predict(model, pca_test)
#summary(test_cases)


```
